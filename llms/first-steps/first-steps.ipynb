{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "625ae3bd-74de-4550-97e2-28dc0c073516",
   "metadata": {},
   "source": [
    "# First steps to becoming a Generative AI Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c4839-e7cb-4094-80dd-8ed55e3f2968",
   "metadata": {},
   "source": [
    "Large language models are systems that receive input (historically only text but nowadays all kind of modalities) and create output. \n",
    "\n",
    "But that is what basically every system does, what makes LLMs special is that they are able to create much more sophisticated outputs than any other system currently on the market. At least in a lot of use-cases.\n",
    "\n",
    "So let's set some of them up and try them out:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a6f01-9d99-4d6f-ab26-fdf560410096",
   "metadata": {},
   "source": [
    "## Running LLMs locally with ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aad69f-4b3f-4fee-96fa-6d0c575ddf39",
   "metadata": {},
   "source": [
    "For this course we assume that you have downloaded ollama locally and pulled/ran \"ollama run gemma3:1b\" once to fetch the smallest gemma 3 model. This model is particularly small that it enables fast iteration locally. It not super smart, but so what :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8e900a-5d92-4505-b695-7c315075ebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Using cached ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpx<0.29,>=0.27 (from ollama)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from ollama)\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting anyio (from httpx<0.29,>=0.27->ollama)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx<0.29,>=0.27->ollama)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<0.29,>=0.27->ollama)\n",
      "  Using cached httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<0.29,>=0.27->ollama)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.29,>=0.27->ollama)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Using cached pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.2)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<0.29,>=0.27->ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached ollama-0.4.8-py3-none-any.whl (13 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Using cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: typing-inspection, sniffio, pydantic-core, idna, h11, certifi, annotated-types, pydantic, httpcore, anyio, httpx, ollama\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 idna-3.10 ollama-0.4.8 pydantic-2.11.3 pydantic-core-2.33.1 sniffio-1.3.1 typing-inspection-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe4056e7-870b-4a6b-a2d8-23f1e1ab7d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there! How’s it going? 😊 \n",
      "\n",
      "What’s on your mind today? Do you want to chat, need help with something, or just want to say hello?\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(model='gemma3:4b-it-qat', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hey you!',\n",
    "  },\n",
    "])\n",
    "# notice your exact answer could be different due to some random processes in the model (learn about that later)\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e6497-0d5a-4d19-9cfe-5b4ba54cb591",
   "metadata": {},
   "source": [
    "As we can see we use the ollama python library to send messages to the ollama model (in our case gemma3:1b). The messages are a list of python dictionaries with each a role and content key. The role is only user and assitant for now where we obviously use user for user messages and assistant for the llm. The content is the generated text/content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f3420-ade5-43ca-b5ae-d0c3eb6ff453",
   "metadata": {},
   "source": [
    "Let's try another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c41b03f-c877-4a39-bd42-02a38339be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Julius! That's fantastic! It’s great to have you join us. Welcome to the course! I'm really excited about this too – it’s a fascinating area. \n",
      "\n",
      "To help me get to know you and what you’re hoping to learn, could you tell me a little more about:\n",
      "\n",
      "* **What are you most interested in learning about specifically within the course?** (e.g., Prompt Engineering, Agent Design, a specific LLM, a particular application like coding or creative writing?)\n",
      "* **What's your background like?** (e.g., Are you a programmer, a writer, a student, someone with no experience, etc.?)\n",
      "* **What are you hoping to *do* with this knowledge?** (e.g., Build your own AI agents, improve your workflow, just understand how these things work?)\n",
      "\n",
      "Don't feel like you have to answer *everything* at once. Just start with whatever comes to mind!  I'm here to help you get the most out of this course. 😊\n"
     ]
    }
   ],
   "source": [
    "response_2 = chat(model='gemma3:4b-it-qat', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hey I am Julius and I am happy to participate in this exciting new llm and ai agents course at opencampus!',\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response_2.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37213d-3583-414a-aca2-83c5024bd1bb",
   "metadata": {},
   "source": [
    "Oh that so nice😊 Let's see if it remembers me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87ceb79-58b4-4824-a23c-86430dcc92c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, this is a classic riddle! I'm going to give you a few clues to help you figure out who you are.  \n",
      "\n",
      "I am:\n",
      "\n",
      "*   **A question.** You asked me this very question.\n",
      "*   **A user.** You are interacting with me.\n",
      "*   **A person seeking information or entertainment.**\n",
      "\n",
      "To help me narrow it down, could you tell me:\n",
      "\n",
      "*   **What are you doing right now?** (e.g., working, relaxing, bored?)\n",
      "*   **What kind of things are you interested in?** (e.g., science, art, food, history?)\n"
     ]
    }
   ],
   "source": [
    "response_3 = chat(model='gemma3:4b-it-qat', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Who am I?',\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response_3.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51212af8-62a6-4acd-8b04-e74407bc1cf7",
   "metadata": {},
   "source": [
    "Oh it forget completely who I was. That's so bad.😭\n",
    "\n",
    "The reason is that the model does not update its weights (i.e. its neuronal connections) permanentially for each of my inputs. So after it produced the answer everything is reset and hence any memory of me is lost.💀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20036c7d-9687-4ce1-9bcd-b8e05704ac67",
   "metadata": {},
   "source": [
    "So to keep the conversation going we have to put all the conversation messages into the messages list in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8948d57a-9533-4db2-84af-1a35edf8198f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Julius! You’re a participant in the exciting new LLM and AI agents course at OpenCampus. 😄 \n",
      "\n",
      "It's good to officially acknowledge that! Are you ready to start learning?\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat(model='gemma3:4b-it-qat', messages=[\n",
    "    {\n",
    "    'role': 'user',\n",
    "    'content': 'Hey I am Julius and I am happy to participate in this exciting new llm and ai agents course at opencampus!',\n",
    "    },\n",
    "    {\n",
    "    'role': 'assistant',\n",
    "    'content': '''Hey Julius! That’s fantastic to hear! It’s great you’re excited about the course. OpenCampus is a really cool platform, and it’s awesome you’re diving in.\\n\\nIt’s great to connect with you too. What specifically are you hoping to get out of the course? Are you interested in learning about LLMs, AI agents, or something else entirely?\\n\\nI'm happy to chat about it – feel free to tell me anything you're thinking. 😊''',\n",
    "    },\n",
    "    {\n",
    "    'role': 'user',\n",
    "    'content': 'Who am I?',\n",
    "    },\n",
    "])\n",
    "\n",
    "print(chat_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633c5a8-26f5-45a8-8dae-e1462f74153d",
   "metadata": {},
   "source": [
    "So now it now I remembers me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2b299-d8fe-4644-bf51-7698f74a1f42",
   "metadata": {},
   "source": [
    "Ok, let's now do some friendly teasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab67126-0b8b-402f-97b6-ed9da10e231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let’s do this! You’ve clearly enjoyed a good, long string of “Hi!” \n",
      "\n",
      "Is there anything specific you’d like to do now, or were you just testing my response to a long sequence?\n"
     ]
    }
   ],
   "source": [
    "tease_response = chat(model='gemma3:4b-it-qat', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 3000*'Hi',\n",
    "  },\n",
    "])\n",
    "\n",
    "print(tease_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82e3cf-f374-44c7-a5ce-11990aeaab5c",
   "metadata": {},
   "source": [
    "As you can see our AI friend is still a bit stupid at some inputs which it has not encountered during training. 3000 \"Hi\"s were recognized as an image..can you maybe think of why? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddea7da-4bf4-4bbd-8c01-fdf3be788d89",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2372b-609c-4485-a900-fdc908bb1c02",
   "metadata": {},
   "source": [
    "Be creative and try to fool/tease the model with inputs it was probably not trained on and see what happens😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8e311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's the weather forecast for Kiel, Germany as of November 2, 2023, at 10:45 AM PST:\n",
      "\n",
      "*   **Current Conditions:** Mostly cloudy\n",
      "*   **Temperature:** 8°C (46°F)\n",
      "*   **Feels Like:** 6°C (43°F)\n",
      "*   **Humidity:** 76%\n",
      "*   **Wind:** West at 13 km/h (8 mph)\n",
      "*   **Visibility:** 10 km (6 miles)\n",
      "*   **Sunrise:** 07:49\n",
      "*   **Sunset:** 16:49\n",
      "\n",
      "**Forecast for today:**\n",
      "\n",
      "*   **High:** 11°C (52°F)\n",
      "*   **Low:** 7°C (45°F)\n",
      "*   **Chance of rain:** 40% mainly in the afternoon.\n",
      "\n",
      "**Source:** [https://www.wetter.de/kiel/](https://www.wetter.de/kiel/)\n",
      "\n",
      "**Would you like me to provide a more detailed forecast (e.g., hourly forecast, extended forecast)?**\n"
     ]
    }
   ],
   "source": [
    "tease2_response = chat(model=\"gemma3:4b-it-qat\", messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'How is the weather today in Kiel'\n",
    "    },\n",
    "])\n",
    "\n",
    "print(tease2_response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33478fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's count the \"r\"s in \"strawberry\"!\n",
      "\n",
      "There are **four** \"r\"s in \"strawberry\". 😊 \n",
      "\n",
      "Let me know if you'd like to try another word!\n"
     ]
    }
   ],
   "source": [
    "tease3_response = chat(model=\"gemma3:1b\", messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'How many r are in strawberry'\n",
    "    },\n",
    "])\n",
    "\n",
    "print(tease3_response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af538e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ytte a se te o tna  \n"
     ]
    }
   ],
   "source": [
    "tease4_response = chat(model=\"gemma3:1b\", messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Write this sentence backwards'\n",
    "    },\n",
    "])\n",
    "\n",
    "print(tease4_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781174b-3b23-4156-a8c9-3c4d8aaab1d3",
   "metadata": {},
   "source": [
    "## Using OpenAI LLMs\n",
    "\n",
    "Let's now try out the models from the company which generated to original buzz around LLMs!\n",
    "(You need your own OPENAI API KEY to play around which this section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec23d0-94d3-4620-8bfe-6115202fc46a",
   "metadata": {},
   "source": [
    "We can use OpenAIs LLMs via two ways:\n",
    "- OpenAI API\n",
    "- Microsoft Azure API\n",
    "\n",
    "So now we using these models as services i.e. they run on somebodyelse computers and we pay for them either with our data or with dollars or with both. Just be aware of it that what your inputting is now transferred to a third-party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fecf01b-5a83-497b-990a-1b4c91c4c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.76.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/nolnmarten/workspace/code/LLM-agents-OC/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Downloading openai-1.76.0-py3-none-any.whl (661 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, jiter, distro, openai\n",
      "Successfully installed distro-1.9.0 jiter-0.9.0 openai-1.76.0 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbcad52f-aa07-4d34-a41e-0ea65b107324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the shimmering moonlight danced on her silver mane, Luna the unicorn soared through the starlit sky, spreading dreams of magic and wonder to every sleeping child on Earth.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a one-sentence bedtime story about a unicorn.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c3451-f6d0-4101-b9d9-f01907202316",
   "metadata": {},
   "source": [
    "So as we can see, the general setup is similar, we also have the messages list. We only need now an API Key to use the external services and have different names from the library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1431d-f7b2-4428-b48f-5b506e74eff2",
   "metadata": {},
   "source": [
    "## Using the free HuggingFace API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37c4a5-f87a-4ea2-a71b-3e8819274532",
   "metadata": {},
   "source": [
    "There is also a free api from HuggingFace which allows you to test your code which models which are freely available. You just need to setup a HuggingFace Account and a READ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d3cc2-2df9-4f51-8d83-9d26a4f39a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Alright, let\\'s tackle this problem step by step. The question is asking: \\n\\n**\"Which number yields the same result when it is added or multiplied with itself?\"**\\n\\nAt first glance, this seems straightforward, but let\\'s break it down to ensure we understand it correctly and arrive at the right answer.\\n\\n### Understanding the Problem\\n\\nWe need to find a number, let\\'s call it **x**, such that:\\n\\n- When you add **x** to itself: **x + x**\\n- When you multiply **x** by itself: **x * x**\\n\\nThese two operations yield the same result. In other words:\\n\\n**x + x = x * x**\\n\\nOur goal is to find the value(s) of **x** that satisfy this equation.\\n\\n### Translating to an Equation\\n\\nLet\\'s write down the equation based on the above:\\n\\n**x + x = x * x**\\n\\nSimplifying the left side:\\n\\n**2x = x²**\\n\\nNow, we have a simple quadratic equation:\\n\\n**x² = 2x**\\n\\n### Solving the Equation\\n\\nTo solve for **x**, let\\'s rearrange the equation to set it to zero:\\n\\n**x² - 2x = 0**\\n\\nThis is a quadratic equation in the standard form **ax² + bx + c = 0**, where:\\n\\n- a = 1\\n- b = -2\\n- c = 0\\n\\nThere are a few methods to solve quadratic equations: factoring, completing the square, or using the quadratic formula. Here, factoring seems straightforward.\\n\\n**Factoring:**\\n\\nFactor out an **x** from both terms:\\n\\n**x(x - 2) = 0**\\n\\nNow, set each factor equal to zero:\\n\\n1. **x = 0**\\n2. **x - 2 = 0** → **x = 2**\\n\\nSo, the solutions are **x = 0** and **x = 2**.\\n\\n### Verifying the Solutions\\n\\nIt\\'s always good practice to verify our solutions by plugging them back into the original conditions.\\n\\n**First solution: x = 0**\\n\\n- Addition: 0 + 0 = 0\\n- Multiplication: 0 * 0 = 0\\n- Are they equal? Yes, 0 = 0.\\n\\n**Second solution: x = 2**\\n\\n- Addition: 2 + 2 = 4\\n- Multiplication: 2 * 2 = 4\\n- Are they equal? Yes, 4 = 4.\\n\\nBoth solutions satisfy the original condition.\\n\\n### Considering the Question\\'s Phrasing\\n\\nThe question asks, \"Which number yields the same result when it is added or multiplied with itself?\" \\n\\nThe use of \"which\" might imply that there\\'s a single answer expected. However, mathematically, we\\'ve found two numbers that satisfy the condition: 0 and 2.\\n\\nIn many contexts, especially in basic arithmetic problems, the number 2 is often the more intuitive answer because adding and multiplying by 2 are common operations where the results coincide (both yielding 4). Zero is a bit of a trivial case since any operation involving zero often results in zero, which might not be as interesting or noteworthy.\\n\\nHowever, mathematically, both are correct. If we\\'re to choose one, it might depend on the context or the level of the problem. For a more general or advanced understanding, acknowledging both solutions is appropriate.\\n\\n### Exploring Further\\n\\nLet\\'s think about why these are the only solutions. \\n\\nOur equation was **x² = 2x**, leading to **x² - 2x = 0**, which factors to **x(x - 2) = 0**. \\n\\nThis gives us two roots: 0 and 2. \\n\\nAre there any other numbers that satisfy this? \\n\\nLet\\'s try another number, say 3:\\n\\n- Addition: 3 + 3 = 6\\n- Multiplication: 3 * 3 = 9\\n- 6 ≠ 9, so 3 doesn\\'t work.\\n\\nTry 1:\\n\\n- Addition: 1 + 1 = 2\\n- Multiplication: 1 * 1 = 1\\n- 2 ≠ 1, so 1 doesn\\'t work.\\n\\nTry -1:\\n\\n- Addition: -1 + (-1) = -2\\n- Multiplication: -1 * -1 = 1\\n- -2 ≠ 1, so -1 doesn\\'t work.\\n\\nThis confirms that only 0 and 2 satisfy the condition among the integers we\\'ve tested. \\n\\nIn fact, since we derived the equation from the general condition and solved it algebraically, we can be confident that these are the only real numbers that satisfy the original statement.\\n\\n### Graphical Interpretation\\n\\nFor a visual understanding, let\\'s plot the two functions:\\n\\n1. **Addition function**: f(x) = x + x = 2x\\n2. **Multiplication function**: g(x) = x * x = x²\\n\\nWe\\'re interested in the points where these two functions intersect, i.e., where f(x) = g(x), which is exactly our original equation.\\n\\nPlotting these:\\n\\n- f(x) = 2x is a straight line passing through the origin with a slope of 2.\\n- g(x) = x² is a parabola opening upwards with its vertex at the origin.\\n\\nThey intersect at:\\n\\n1. x = 0: f(0) = 0, g(0) = 0\\n2. x = 2: f(2) = 4, g(2) = 4\\n\\nThis graphical approach confirms our algebraic solution.\\n\\n### Alternative Approach: Rearranging the Equation\\n\\nAnother way to approach the equation **x² = 2x** is to divide both sides by **x**, but we must be cautious because division by zero is undefined.\\n\\nAssuming **x ≠ 0**, we can divide both sides by x:\\n\\n**x = 2**\\n\\nThis gives us one solution, x = 2. \\n\\nBut we know from earlier that x = 0 is also a solution. By dividing by x, we implicitly assumed x ≠ 0, which is why we missed the x = 0 solution. \\n\\nThis highlights the importance of considering all possibilities when solving equations, especially when operations like division are involved.\\n\\n### Conclusion\\n\\nAfter carefully analyzing the problem through algebraic manipulation, verification of solutions, testing other numbers, and even a graphical approach, we\\'ve determined that there are two numbers that satisfy the condition where adding the number to itself is the same as multiplying the number by itself.\\n\\n**Final Answer:** The numbers that yield the same result when added to themselves or multiplied by themselves are **0** and **2**. \\n\\nHowever, if the expectation is for a single, non-trivial answer, then **2** is the number that satisfies this condition (since 0 + 0 = 0 * 0 = 0, and 2 + 2 = 2 * 2 = 4). \\n\\nBoth are mathematically correct, but the more commonly intended answer in such puzzles is **2**.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/novita/v3/openai/chat/completions\"\n",
    "headers = {\"Authorization\": f\"{API_KEY}\"}\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Which number yields the same result when it is added or multiplied with itself?\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"deepseek/deepseek-v3-0324\",\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=payload)\n",
    "print(response.json()[\"choices\"][0][\"message\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe53eb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Alright, let\\'s tackle the problem: **How many \\'G\\'s are in the word \\'huggingface\\'?**\\n\\n### Understanding the Problem\\nFirst, I need to determine how many times the letter \\'G\\' (both uppercase and lowercase, though in this case, the word is all lowercase) appears in the word \"huggingface.\"\\n\\n### Breaking Down the Word\\nLet\\'s write out the word and look at each letter one by one.\\n\\nThe word is: h u g g i n g f a c e\\n\\nNow, let\\'s list the letters with their positions to keep track:\\n\\n1. h\\n2. u\\n3. g\\n4. g\\n5. i\\n6. n\\n7. g\\n8. f\\n9. a\\n10. c\\n11. e\\n\\n### Counting the \\'G\\'s\\nNow, let\\'s go through each letter and count how many times \\'g\\' appears.\\n\\n1. h - not a g\\n2. u - not a g\\n3. g - this is a g (count = 1)\\n4. g - another g (count = 2)\\n5. i - not a g\\n6. n - not a g\\n7. g - another g (count = 3)\\n8. f - not a g\\n9. a - not a g\\n10. c - not a g\\n11. e - not a g\\n\\n### Verifying\\nLet me recount to ensure I didn\\'t miss anything:\\n\\n- Third letter: g (1)\\n- Fourth letter: g (2)\\n- Seventh letter: g (3)\\n\\nNo other \\'g\\'s are present in the word.\\n\\n### Potential Pitfalls\\nIt\\'s easy to overlook letters, especially when they\\'re repeated or not at the beginning. One might rush and miss the \\'g\\'s in the middle or at the end. Also, ensuring that we\\'re only counting lowercase \\'g\\'s is important unless specified otherwise (here, the word is all lowercase).\\n\\n### Conclusion\\nAfter carefully examining each letter in \"huggingface,\" I found that the letter \\'g\\' appears **three times**.\\n\\n### Final Answer\\nThere are **3 \\'G\\'s** in the word \\'huggingface\\'.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/novita/v3/openai/chat/completions\"\n",
    "headers = {\"Authorization\": f\"{API_KEY}\"}\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many 'G's in 'huggingface'?\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"deepseek/deepseek-v3-0324\",\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=payload)\n",
    "print(response.json()[\"choices\"][0][\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a733e72-b349-48a2-a5b8-20eabe8b1058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle the problem: **How many 'G's are in the word 'huggingface'?**\n",
       "\n",
       "### Understanding the Problem\n",
       "First, I need to determine how many times the letter 'G' (both uppercase and lowercase, though in this case, the word is all lowercase) appears in the word \"huggingface.\"\n",
       "\n",
       "### Breaking Down the Word\n",
       "Let's write out the word and look at each letter one by one.\n",
       "\n",
       "The word is: h u g g i n g f a c e\n",
       "\n",
       "Now, let's list the letters with their positions to keep track:\n",
       "\n",
       "1. h\n",
       "2. u\n",
       "3. g\n",
       "4. g\n",
       "5. i\n",
       "6. n\n",
       "7. g\n",
       "8. f\n",
       "9. a\n",
       "10. c\n",
       "11. e\n",
       "\n",
       "### Counting the 'G's\n",
       "Now, let's go through each letter and count how many times 'g' appears.\n",
       "\n",
       "1. h - not a g\n",
       "2. u - not a g\n",
       "3. g - this is a g (count = 1)\n",
       "4. g - another g (count = 2)\n",
       "5. i - not a g\n",
       "6. n - not a g\n",
       "7. g - another g (count = 3)\n",
       "8. f - not a g\n",
       "9. a - not a g\n",
       "10. c - not a g\n",
       "11. e - not a g\n",
       "\n",
       "### Verifying\n",
       "Let me recount to ensure I didn't miss anything:\n",
       "\n",
       "- Third letter: g (1)\n",
       "- Fourth letter: g (2)\n",
       "- Seventh letter: g (3)\n",
       "\n",
       "No other 'g's are present in the word.\n",
       "\n",
       "### Potential Pitfalls\n",
       "It's easy to overlook letters, especially when they're repeated or not at the beginning. One might rush and miss the 'g's in the middle or at the end. Also, ensuring that we're only counting lowercase 'g's is important unless specified otherwise (here, the word is all lowercase).\n",
       "\n",
       "### Conclusion\n",
       "After carefully examining each letter in \"huggingface,\" I found that the letter 'g' appears **three times**.\n",
       "\n",
       "### Final Answer\n",
       "There are **3 'G's** in the word 'huggingface'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uhh let's render this long markdown string to read it more easily\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(response.json()[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07533d46-4b85-439d-a6f9-a50bb2a66cd8",
   "metadata": {},
   "source": [
    "Really cool, isn't it? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20558ca8-a69c-497c-89cc-07d426319300",
   "metadata": {},
   "source": [
    "### Short excurse about the term payload\n",
    "\n",
    "I wondered always why they use the term **payload**. So here is a nice explanation: \n",
    "\n",
    "In API requests, the term payload refers to the **actual data** or information being sent within the request, typically in the body of an HTTP request (such as POST or PUT). The term is derived from communication and networking protocols, where the payload is the part of transmitted data that contains the meaningful information, as opposed to headers or metadata which describe or route the data.\n",
    "\n",
    "Why is the term \"payload\" commonly used?\n",
    "\n",
    "Historical Origin:\n",
    "The term originally comes from aerospace and transport, describing the **actual useful content** carried by a vehicle (like cargo in a truck or spacecraft).\n",
    "\n",
    "Clear Differentiation:\n",
    "It clearly distinguishes between control data (headers, parameters, routing information) and the actual content (payload) intended for processing by the receiving application or service.\n",
    "\n",
    "Generalization:\n",
    "Payload provides a generic, neutral term for the transmitted data, regardless of its structure (JSON, XML, plain text, binary files, etc.), which simplifies communication across different APIs and systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ea1af-5f07-4344-a762-faf96d4ab47f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So there are a lot different LLMs out in the wild. They differ in size and capabilities. We always access them via an API call. They run either locally, in our own cloud or we use some third party provider. Always check where the data is going and what implactions that might have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b24c5-ac06-4f70-86e3-295149469714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
