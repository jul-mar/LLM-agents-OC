{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d892af8-7665-43c4-bce0-bfd835072b69",
   "metadata": {},
   "source": [
    "# First langchain tutorial\n",
    "\n",
    "Langchain is arguably the most famous framework for building first llm applications. This is also why we use it as a start for our journey. If you are more experienced -> don't worry the course will scale up in complexity quite soon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946d49c-9c87-47bd-b7ab-c08108b2c78c",
   "metadata": {},
   "source": [
    "## Build a simple LLM application with chat models and prompt templates\n",
    "\n",
    "In this first tutorial we'll see how to build a simple LLM application with LangChain. This application will translate text from English into German. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7866c-6622-4ee3-a6bd-46884acf0252",
   "metadata": {},
   "source": [
    "First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to [supported integrations](https://python.langchain.com/docs/integrations/chat/).\n",
    "We will stick with ollama for now and our small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6efdc342-ffda-4122-9206-4d5ea76bec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -qU \"langchain[ollama]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c1a254-87ed-475f-856a-5c8dbb24f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemma3:4b-it-qat\", model_provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5c498-e802-4a14-8561-0078ae99f180",
   "metadata": {},
   "source": [
    "Let's first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a4f0ed-dc44-461a-b677-a2dc071c4881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are a few good options, depending on the context and how friendly you want to sound:\\n\\n* **Hallo Deutschland!** (Most common and generally appropriate) - This is the standard, straightforward translation.\\n* **Hallo, Deutschland!** (Also very common) - This is slightly more informal.\\n* **GrÃ¼ÃŸ dich, Deutschland!** (More friendly and enthusiastic) - This is a more heartfelt greeting, suggesting a warm welcome.\\n\\n**Which one should you use?**\\n\\nGenerally, **Hallo Deutschland!** is a safe and perfectly acceptable choice. ðŸ˜Š\\n\\nWould you like me to translate something else?', additional_kwargs={}, response_metadata={'model': 'gemma3:4b-it-qat', 'created_at': '2025-04-23T19:54:06.128772621Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7622557511, 'load_duration': 4373482598, 'prompt_eval_count': 24, 'prompt_eval_duration': 376537698, 'eval_count': 129, 'eval_duration': 2839940804, 'model_name': 'gemma3:4b-it-qat'}, id='run-4337f672-b75f-47cd-81e7-fa9cb2773a26-0', usage_metadata={'input_tokens': 24, 'output_tokens': 129, 'total_tokens': 153})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Translate the following from English into German\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello Germany!\"},\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb532d6f-ac0d-43fe-8d8b-75d74c86dadd",
   "metadata": {},
   "source": [
    "Currently we are creating and passing the messages directly to the model. For making this more general we introduce chat templates which are basically those messages containing certain values which we are setting dynamically. Right now it is not important but later for building a real chat it becomes important.\n",
    "\n",
    "Prompt templates are a concept in LangChain designed to assist for this. They take in raw user input and return a prompt that is ready to pass into a language model.\n",
    "\n",
    "Let's create a prompt template here. It will take in two user variables:\n",
    "\n",
    "language: The language to translate text into\n",
    "\n",
    "text: The text to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "510066d0-219f-4ab0-8035-283f367b2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e6dbdc-9efa-41ea-99f6-49e5351c624e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into German', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello Germany!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"German\", \"text\": \"Hello Germany!\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6923cd4-a8a1-4805-bb5b-fdf460e17375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few options, depending on the context and how formal you want to be:\n",
      "\n",
      "*   **Hallo Deutschland!** (Most common and versatile - simply \"Hello Germany!\")\n",
      "*   **Hallo Deutschland!** (Same as above, but with a slightly more emphasized pronunciation of \"Deutschland\")\n",
      "\n",
      "You could also say:\n",
      "\n",
      "*   **Guten Tag, Deutschland!** (Good day, Germany!) - More formal\n",
      "\n",
      "I'd recommend **Hallo Deutschland!** for most situations.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58307a-258f-4487-99cf-129002e5245e",
   "metadata": {},
   "source": [
    "**Note:** Langchain templates are just a way to do some basic string and object manipulation, it's something you could also do by hand as we did in the first cell of this notebook. It's just langchains idea how to package that into some format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680af6d6-0b17-4711-8185-c9991029fbb8",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "Let's now do the same with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78884d62-fdaf-4e8f-8316-0391e8842cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hallo Deutschland!\n"
     ]
    }
   ],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\n",
    "#         prompt=\"Enter your OpenAI API key (required if using OpenAI): \"\n",
    "#     )\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# .env laden\n",
    "load_dotenv()\n",
    "\n",
    "# API Key aus Umgebungsvariable holen\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4e84370-3ca1-4c70-978d-1dd367c72cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI LLM mit Key verwenden\n",
    "llm = OpenAI(openai_api_key=api_key, temperature=0.7)\n",
    "\n",
    "# Prompt und Chain bauen\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"language\", \"text\"],\n",
    "    template=\"Translate the following from English into {language}: {text}\",\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b94d0667-dc4c-43ae-87a8-8cb1d8b2e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hallo Deutschland!\n"
     ]
    }
   ],
   "source": [
    "# Testlauf\n",
    "antwort = chain.run({\n",
    "    \"language\": \"german\",\n",
    "    \"text\": \"Hello Germany!\"\n",
    "})\n",
    "print(antwort)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf71f2-11be-4aad-9666-8c5d60123ad6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Different LLMs do similar things, what differs is how good they are in doing the task. For this simple translation obviously every LLM is able to do it. For more complex tasks you have to see which LLMs is able to do it. We will take a look at that later in the course. Personally I think of different LLMs like different employees -> some are generally smarter, some are better at a certain task and worse at another compared to other employees, at the end it comes down that we test those LLMs on the task at hand and see which ones are able to solve it. Similarly we test humans (exams, assesments centers, ...) if they are able to the task and which one does it best :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337951d-cd02-45b9-9a71-a48cd67a7e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
